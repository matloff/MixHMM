
\documentclass[11pt]{article}

\include{Includes} 

\begin{document} 

\title{Mixture and Hidden Markov Models: a Unified Introduction}
\author{Norman Matloff \\
   Department of Computer Science \\
   University of California, Davis}

\maketitle

The notion of \textit{mixture models} (MMs)  is a classical
probabilistic concept, and arises frequently in applications.  The field
of \textit{Hidden Markov Models} (HMMs) is also a quite well established
probabilistic model, but has received much more attention with the rise
of interest in machine learning.  HMMs too have very interesting
applications, such as in bioinformatics and language processing.

As there is a natural connection of mixture models (MMs) to HMMs, we
present both here.  We also present examples of using R packages to
apply these models to data.

We will cover enough mathematical detail to specify the models, but
subject to the goal of keeping things simple.  In the HMM case in
particular, derivations generally involve some famous algorithms that
will not be covered in detail here.

\section{Overview}

Here we will present two examples, and then give a high-level view of
the structures and issues are in MMs and HMMs.  In both models, we have
an observed variable $Y$, and a hidden or \textit{latent} state $S$.

\subsection{Motivating Example:  Box of Batteries}

Say we have a large box of batteries.  They are known to be of two
different types, but the two types are visually indistinguishable.
From past experience, suppose we know that a good model for lifetimes of
batteries is exponential.  We have three unknown parameters: $q$ the
proportion of batteries of the first type; $\mu_1$, the mean lifetime of
the first type, and $\mu_2$, the mean for the second type.

We take a random sample of $n$ batteries and measure the lifetimes of
each, resulting in our data $Y_1,...,Y_n$, independent and identically
distributed (iid) random variables.  Unseen are the types of these
batteries, $S_1,...,S_n$, the \textit{hidden state} of each battery.

Our objective is the estimate $q$ and the $\mu_i$, based on the $Y_j$.  

\subsection{Motivating Example:  Network Noise}

Suppose we have a network line that is known to occasionally be
noisy, and that during noisy periods bits will be corrupted in such
a way that the probability of a 0, which is 0.5 in the original
transmitted data, is 0.20 during noisy periods.  Suppose that on average
10\% of the bits arrive during noise periods.

We focus on the situation in which the status of the line, working vs.\
noisy, is unknown.  It thus is a \textit{hidden state}.  We have data
$Y_j$ on the received bits, and want to estimate $O_j$, the originally
sent bits.

\subsection{Motivating Example:  Old Faithful Geyser}

The data here consist of durations of eruptions of the famous
Old Faithful geyser in the US' Yellowstone National Park.  The dataset
is \textbf{faithful}, a built-in dataset in R.

A histogram, obtained via 

\begin{lstlisting}
> hist(faithful$eruptions)
\end{lstlisting}

and shown in Figure \ref{faithfulhist}, seems to suggest that the
eruption duration distribution is a mixture of two normally distributed random
variables.  This seems even more plausible if we use R's 
\textbf{density()} function, as in Figure
\ref{faithfulhistsmooth}.\footnote{A histogram is a probability density
estimate (as long as one keeps the total area under the curve to be 1.0,
as we have done here by setting \textbf{freq} to FALSEi).  More advanced
density estimators, such as produced by R's \textbf{density()} function,
produce smoother and potentially more accurate plots.  Such methods have
parameters analogous to the number of breaks/bins in a histogram; for
R's \textbf{density()} function, the argument is the bandwidth
\textbf{bw}, which we have taken to be the default here.}

This has led to many physical theories over the years.  Rather elaborate
physical models have been developed, such as that in O'Hara and Esawi,
Model for the eruption of the Old Faithful geyser, Yellowstone National
Park, \textit{GSA Today}, June 2013.  This paper is full of physical
detail (``... the dynamics of vapor bubble formation (and collapse)
during boiling in the conduit...''), but in simple terms, it posits two
processes, which gave rise to long and short durations before an
eruption, consistent with the bimodal density form suggested by the
above graphs.

\begin{figure}[tb]
\centerline{
\includegraphics[width=3.0in]{FaithfulDuration.png}
}
\caption{Old Faithful eruption durations}
\label{faithfulhist}
\end{figure}

\begin{figure}[tb]
\centerline{
\includegraphics[width=3.0in]{FaithfulDurationSmooth.png}
}
\caption{Old Faithful eruption times, smoothed}
\label{faithfulhistsmooth}
\end{figure}

Assuming there really are two types of eruptions, our hidden state $S_j$
for the $j^{th}$ eruption in our dataset is the type of eruption.  $Y_j$
is the duration of that eruption.  Again, the $S_j$ are unobserved.

Our objective is to use the $Y_j$ data to estimate $q$, the proportion
of type 1 eruptions, and
$
\mu_1,
\mu_2,
\sigma_1 
\textrm{ and }
\sigma_2 
$, the means and standard deviations of the assumed normal distributions
for duration in the two eruption types.

% \subsection{``Hidden'' States}
% 
% In the network example, the hidden state is whether we are currently in
% a clear period (coded 1) or a noisy period (coded 2).  The observed
% state is the actual received bit.
% 
% For the purposes of the discussion here, let's take as our geyser model
% that of O'Hara \textit{et al} cited above.  Our \textit{hidden}, i.e.\
% unknown, state here will be the identity of the currently eruption type,
% long (coded 1) or short (coded 2) Our \textit{observed} state is the
% duration of the eruption.  Note the overlap: some durations
% for type 1 eruptions may actually be shorter than some durations
% for eruptions of type 2.

\subsection{Number of States}

In many applications, a major part of the modeling process is deciding
on the number of states.  In our models above, it is natural to take
this number to be 2, but generally there is no obvious such number.  In
such cases, we have a classic model-fitting choice, a famous principle
in model fitting:

\begin{quote}

The \textit{Bias-Variance Tradeoff}.  

Say we wish to predict human weight from height.  We wish to estimate
the function $w(t) = E(W | H = t)$, the relation between weight and
height in our population.  We might try a linear model for $w(t)$, but a
quadratic model would give use greater flexibility.  A cubic model be
even more general, and so on.

From the notion of a Taylor series in calculus\footnote{Or for those
with a background in real analysis, the Stone-Weierstrass Theorem.},
one might think that the higher the degree of the fitted polynomial, the
better.  But that is merely saying that higher-degree models have
smaller model bias, and counteracting that is the problem of sampling
error.  The higher the degree, the more the sampling error (called the
\textit{standard error} in statistics).  So, we need larger datasets
for higher-degree models.

If we are on the ``wrong'' side of this tradeoff, the model is said to
be \textit{overfit}.

\end{quote}

So, the more hidden states in our model, the smaller the model bias but
the larger the ampling error; setting the number of hidden states at too
large a level will result in overfitting.

For instance, consider financial time series data, such as daily stock
market data.  The \textbf{sp500} dataset, included with some software we
will use below, consists of 772 days of the Standard and Poor market
average.  In the book associated with the software,\footnote{\textit{Mixture and
Hidden Markov Models with R,} by Ingmar Visser and Maarten
Speekenbrink, Springer.} the authors postulate 2 hidden states, ``bull''
and ``bear'' sentiments among the traders, and fit an HMM accordingly.
With 3 states, 4 states and so on, we might achieve more accuracy, 
but eventually the Bias-Variance Tradeoff will result in overfitting.

Simiarly, the \textbf{faithful} dataset consists of only 272
observations, for instance.  And though the choice of 2 states in that
example does not seem large relative to 272, we will see below that we
are maximizing a certain quantity over an enormous number of choices,
again raising the possibility of overfitting.

\subsection{Time (and ``Time'')}

In spite of sharing the property of having hidden states, and the $Y|S$
structure, there is a fundamental difference between the battery example
above and the other two.  In the battery case, the pairs $(S_j,Y_j)$ are
iid, while in the network and geyser examples they are sequential in
time; there is a time dependency between each pair and the next.  The
stock market example is also sequential.

In some cases ``time'' may correspond to postion, say in HMM genomics
models, but again, the sequential nature is key.  That is what
distinguishes HMMs from MMs.

\subsection{Conditional Probabilities of Observed Values}

Let $S$ denote the hidden state, and let $Y$ denote the corresponding
observed value.  A major ingredient in the analysis will be expressions of
the form

\begin{equation}
P(Y = w | S  = v)
\end{equation}

If $Y$ is continuous rather than discrete, the above expression would be
something like

\begin{equation}
f_{Y|S} (w,v) 
\end{equation}

where $f_{Y|S}$ is the conditional density of $Y$ given $S$:

\begin{equation}
f_{Y|S}(w,v) 
= \frac{d}{dw} F_{Y|S}(w,v) 
= \frac{d}{dw} P(Y \leq w ~|~ S = v)
\end{equation}

These conditional distribution quantities are then used to estimate
model parameters, as we will see below.\footnote{The word
\textit{estimate} is vital here.  As with any statistical method, our
results are just estimates of population values, and the larger our
sample, the more likely our estimate is to the true value.}

\subsection{Mean and Variance of Random Variables in Latent-State Models}
\label{mixmeanvar}

$EY$ follows the Law of Total Expectation:

\begin{equation}
\label{mixmean}
EY = E[E(Y | S)]
\end{equation}

Of course, evaluating this would require being able to compute E(Y $|$
S), which is easy in some cases, not so easy in others.

Also, we have the Law of Total Variance, 

\begin{equation}
\label{mixvar}
Var(Y) = E[Var(Y|S)] + Var[E(Y|S)]
\end{equation}

% \subsection{Mixture Models vs.\ HMMs: the 10,000 Foot View}
% 
% Both types of models have hidden and observed states.  But HMMs add an
% extra aspect:  We model the system as evolving in time, with
% probabilities of going from one state to another at each time step.  So
% this is a dynamic model, while MMs are static. 
% 
% In the Old Faithful example, for instance, in an MM we may wish to
% estimate the overall proportion of eruptions of type 1, while in an
% HMM we may wish to have an estimated probability that the \textit{next}
% eruption is of that type.
% 
% Another example is a financial dataset included in the HMM software we will
% use.  The (hypothetical) hidden state is whether the stock market
% sentiment is in a ``bull'' or ``bear'' mood.  The market, of course,
% definitely has a time component, daily, hence the HMM analysis.  But the
% authors also do a static analysis, looking at the data as a whole in a
% way similar to the static geyser analysis.

\section{Mixture Models}

Actually, MMs are typically not presented in the conditional distribution
form we saw above.  Let's see how to reconcile the standard description
with what we saw above.

\subsection{Definition}
\label{mixdef}

Say $Y$ is discrete.  Then

\begin{equation}
P(Y = w) = \sum_{v} P(S = v) ~ P(Y = w | S = v)
\end{equation}

So in terms of cumulative distribution functions (cdfs),

\begin{equation}
\label{mixedFs}
F_Y(w) = P(Y \leq w) = 
\sum_{v} P(Y \leq w | S = v) ~ P(S = v) =
\sum_{v} q_v F_{Y|S}(w,v)
\end{equation}

where 

\begin{equation}
q_v = P(S = v)
\end{equation}

Speaking just in terms of cdfs, we say that $F_Y$ is a \textit{mixture}
of the cdfs $F_{Y|S}$, which simply means that $F_Y$ is a linear
combination of the $F_{Y|S}$, where the coefficients are nonnegative
numbers whose sum is 1.\footnote{In math parlance, we say that $F_Y$ is
a \textit{convex} combination of the $F_{(Y|S)}$..}

We may have mixtures of more than two distributions.  Consider random
variables $X_1,...,X_k$, with cdfs $F_{X_i}$ (not necessarily
independent),\footnote{I use ``X'' for my variable name rather than
``Y,'' to emphasize the mainly implicit nature of states.} and let $d_i,
~ i=1,...,k$ be nonnegative numbers whose sum is 1.  Define the random
variable $W$ to take on the value $X_i$ with probability $q_i$,
$i=1,...,k$.  Then we say that $W$ has a mixture distribution.  With
this view, the state is only implicit.

Note that

\begin{equation}
F_{W}(t) = \sum_{i=1}^k q_i F_{X_i}(t)
\end{equation}

Similar relations hold for probability mass functions ($X_i$ discrete)
and density functions ($X_i$ continuous).

In MM analysis, the usual quantities of interest are the 
$F_{X_i}$ and the $q_i$.  In the Old Faithful example, the former 
typically means using the data to somehow estimate the means and
standard deviations of the two normal distributions, and the proportions
of eruptions of the two types.

Denote the mean and variance of $X_i$ here by $\mu_i$ and $\sigma_i^2$
(whether or not the $X_i$ have a normal distribution).  Then
(\ref{mixmean}) and (\ref{mixvar}) become

\begin{equation}
\label{MMexpmean}
EY = \sum_{i=1}^k q_i \mu_i
\end{equation}

and

\begin{equation}
\label{MMexpvar}
\sum_{i=1}^k q_i \sigma_i^2 +
\sum_{i=1}^k q_i (\mu_i - \hat{\mu})^2
\end{equation}

where

\begin{equation}
\hat{\mu} = \frac{1}{k} \sum_{i=1}^k \mu_i
\end{equation}

\subsection{Cluster Analysis}

The $X_i$ can be vector-valued, and this is the basis for
\textit{cluster analysis}, a major field in machine learning/data
science.  There we wish to informally divide our dataset into a few
groups, thus a mixture problem.  One of the key issues is the number of
groups to postulate, again deterimined informally.  See
\textit{https://cran.r-project.org/web/views/Cluster.html} for an
extensive choice of R libraries.

\subsection{The EM Algorithm}

Probably the most common approach to estimating such quantities is the
\textit{EM algorithm}.  The details can become complex, but let's at
least look at an overview here.

Say the distribution of some probabilistic quantity, depends on two sets
of parameters, say $\theta$ and $\gamma$, the first describing the
distribution of $Y|S$ and the second describing the distribution of $S$,
that is, consisting of the mixing proportions $q_i$.  (We only need
$q_i,q_2,...,q_{k-1}$, since the proportions must sum to 1.)

In the geyser example, $\theta$ is consists of the two means $\mu_i$ and
two standard deviations $\sigma_i$ of the two normal distributions, and
$\gamma$ is the proportion $q$ of the the type 1 eruptions.  

The algorithm works like this:  We set initial guesses, $\theta_0$ and
$\gamma_0$ for the two parameter sets, then update alternately, first
finding a new guess for $\theta$ based on our latest guess for $\gamma$,
then vice versa.  Of course, we also make use of our dataset at every
step.  So, the core of the algorithm is to iterate the following for
$i=1,2,3,...$ until convergence:

\begin{enumerate}

\item Form a new guess for $\theta$, denoted $\theta_{i+1}$, based
on $\gamma_i$.

\item Form a new guess for $\gamma$, denoted $\gamma_{i+1}$, based
on $\theta_{i+1}$.

\end{enumerate} 

How does this work in the geyser example?  

\textbf{Step 1.}  The `M' in ``EM'' stands for ``maximization,''
alluding to the famous statistical estimation tool, Maximum Likelihood
Estimation, familiar to many readers of this tutorial.  Note that the
likelihood is calculated using the density of $Y$, which is

\begin{equation}
f_Y(t) = 
q \left [ \frac{1}{\sqrt{2\pi} \sigma_1}
\exp \left ( \frac{t-\mu_1}{\sigma_1} \right)^2 \right ] + 
(1-q) \left [ \frac{1}{\sqrt{2\pi} \sigma_2}
\exp \left ( \frac{t-\mu_2}{\sigma_2} \right)^2 \right ]
\end{equation}

Remember, in every Step 1, $q$ is considered known.  We maximize with
respect to the $\mu_j$ and $\sigma_j$.

\textbf{Step 2.}  Since $k=2$, (\ref{MMexpmean}) becomes

\begin{equation}
\label{step2}
EY = q \mu_1 + (1-q) \mu_2
\end{equation}

Remember, in every Step 2, the $\mu_j$ (and the $\sigma_j$) are
considered known.  And, we can estimate $EY$ in the left side
(\ref{step2}) by the mean $Y$ value in our dataset.  So, we then simply
solve for $q$ to obtain the latest iterate for $q$.\footnote{For readers who
know the Method of Moments estimation tool, the Step 2s, the `E' steps,
can be viewed roughly as using that tool.  For cases with general $k$,
we need to estimate $k-1$ moments.}

\subsection{The mixtools Package}

This is a large package with many functions for analysis of MMs.  The EM
algorithm is used extensively.  Here we will illustate the function
\textbf{normalmixEM()}, which as the name implies, fits an MM of normal
distributions.  Again, for the sake of simplicity, we will cover only a
few of the many features of this function.

The algorithm is iterative, and thus requires initial guesses for the
means/standard deviations of the two normal distributions, and the
proportions of the two eruption types.  I took the former (arguments
\textbf{mu} and \textbf{sigma}) from the appearance of the histogram,
and used equal weights for the latter (argument \textbf{lambda}).

\begin{lstlisting}
> mixout <- normalmixEM(faithful$eruptions,
   lambda=0.5,mu=c(55,80),sigma=10,k=2) 
number of iterations= 7
> str(mixout)
List of 9
 $ x         : num [1:272] 3.6 1.8 3.33 2.28 4.53 ...
 $ lambda    : num [1:2] 0.36 0.64
 $ mu        : num [1:2] 2.05 4.3
 $ sigma     : num [1:2] 0.364 0.364

 ...
\end{lstlisting}

So, the estimate from EM is that about 36\% of the eruptions are of type
1.  The estimated mean eruption durations for the two eruption types
are 2.05 and 4.3.  (My initial guess for the standard deviations, 1.0,
was about 3 times too high.)

\subsection{Vector-Valued X}

The most common mixture modeling is in \textit{cluster analysis}, often
referred to as \textit{unsupervised learning}.  We have multivariate
data, say in a marketing application, and wish to find meaningful
subgroups, say different types of customers.  Again, we don't know what
types are there, if there are any in some sense, but if we can find
some, this may be very useful.

The \textbf{faithful} data is bivariate, with columns for both eruption
duration and waiting time between eruptions.  A plot, seen in Figure
\ref{bivariatefaithful}, does seem to show two groups.  Of course, if
there really are two groups, we can't tell for sure here which point
belongs to which group, but again, in say, a marketing context, we just
want to identify rough groups.  

\begin{figure}[tb]
\centerline{
\includegraphics[width=3.0in]{BivarFaithful.png}
}
\caption{Old Faithful bivariate plot}
\label{bivariatefaithful}
\end{figure}

In the univariate case, we assumed normal distributions for the
components.  The \textbf{mixtools} function \textbf{mvnormalmixEM()} 
fits a multivariate normal model.  I tried running it completely on the
basis of the argument defaults:

\begin{lstlisting}
> mvnout <- mvnormalmixEM(fMaithful)
number of iterations= 12 
> str(mvnout)
List of 9
 $ x         : num [1:272, 1:2] 3.6 1.8 3.33 2.28 4.53 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:272] "1" "2" "3" "4" ...
  .. ..$ : chr [1:2] "eruptions" "waiting"
 $ lambda    : num [1:2] 0.356 0.644
 $ mu        :List of 2
  ..$ : num [1:2] 2.04 54.48
  ..$ : num [1:2] 4.29 79.97
 $ sigma     :List of 2
  ..$ : num [1:2, 1:2] 0.0692 0.4352 0.4352 33.6973
  ..$ : num [1:2, 1:2] 0.17 0.941 0.941 36.046
...
\end{lstlisting}

Since our data is bivariate, the estimated mean for each cluster has
is a vector of length 2, with a 2 $\times$ 2 covariance matrix.  The
estimates are displayed in the above output.  The \textbf{lambda} output
again shows the estimated mixing proportions, similar to the ones we
found in the univariate case.

Cluster analysis is a vast topic.  Estimation via the EM algorithm is
only one of many methods to choose from.

\subsection{Overdispersion Models}

Recall the following about the Poisson distribution family:

\begin{itemize}

\item [(a)] This family is often used to model counts.

\item [(b)] For any Poisson distribution, the variance equals the mean.

\end{itemize}

In some applications in which we are modeling count data, condition (b)
is too constraining.  We want a ``Poisson-ish'' distribution in which
the variance is greater than the mean, called an {\it overdispersion}
model.

One may then try to fit a mixture of several Poisson distributions,
instead of a single one.  This does induce overdispersion, as we will
now see.  

The states here will be totally fictitious, just a vehicle to achieve an
overdispersed model.  Say the distribution of $Y$ given $S = i$ is
Poisson with parameter $\lambda_i, i = 1,2,...,k$.  Then $Y$ has a
mixture distribution.  Our goal here will be to show that $Y$ is indeed
overdispersed, i.e. has a large variance than mean.

By the Law of Total Expectation (\ref{MMexpmean})

\begin{eqnarray}
Var(Y) &=& E[Var(Y|S)] + Var[E(Y|S)] \\ 
&=& E(\lambda_S) + Var(\lambda_S) \label{thislast} \\
&=& EY + Var(\lambda_S) \label{thislast} 
\end{eqnarray}

% \begin{equation}
% \label{meanlamb}
% EY = \sum_{i=1}^k q_i \lambda_i
% \end{equation}

Note that in the above, the expression $\lambda_S$ is a random variable,
since its subscript S is random.  The random variable $\lambda_S$ takes
on the values $\lambda_1,...,\lambda_k$ with probabilities
$q_1,...,q_k$.

% \begin{eqnarray}
% Var(Y) &=& E[Var(Y|S)] + Var[E(Y|S)] \\ 
% &=& E(\lambda_S) + Var(\lambda_S) \label{thislast} \\
% &=& EY + Var(\lambda_S) \label{thislast} 
% \end{eqnarray}

Did you notice that this last equation achieves our goal of showing
overdispersion?  Since

\begin{equation}
Var(\lambda_S) > 0
\end{equation}

we have that

\begin{equation}
Var(Y) > EY
\end{equation}

exactly the definition of overdispersion.

% But let's see just how much greater the variance is than the mean.  The
% second term in (\ref{thislast}) is evaluated the same way as in
% (\ref{elambm}):  This is the variance of a random variable that takes on
% the values $\lambda_1,...,\lambda_k$ with probabilities $p_1,...,p_k$,
% which is
% 
% \begin{equation}
% \sum_{i=1}^k p_i (\lambda_i - \overline{\lambda})^2
% \end{equation}
% 
% where 
% 
% \begin{equation}
% \overline{\lambda} =  E\lambda_S = \sum_{i=1}^k p_i \lambda_i
% \end{equation}
% 
% Thus
% 
% \begin{equation}
% EY = \overline{\lambda}
% \end{equation}
% 
% and
% 
% \begin{equation}
% Var(Y) = \overline{\lambda} + 
% \sum_{i=1}^k p_i (\lambda_i - \overline{\lambda})^2
% \end{equation}
% 
% So, as long as the $\lambda_i$ are not equal, we have
% 
% \begin{equation}
% Var(Y) > EY
% \end{equation}

So, if one has count data in which the variance is greater than the
mean, one might try using this model.

In mixing the Poissons, there is no need to restrict to discrete S.  In
fact, it is not hard to derive the fact that if X has a gamma
distribution with parameters r and p/(1-p) for some $0 < p < 1$, and Y
given X has a Poisson distribution with mean X, then the resulting Y
neatly turns out to have a negative binomial
distribution.\footnote{Recall that this distribution family arises as
the number of trials, e.g.\ number of coin flips, needed to accumulate
$m$ succeses, e.g.\ $m$ heads.}  In other words, the negative binomial
family also has the overdispersion property.

\section{Hidden Markov Models}

As noted, we have an observable variable $Y$, and a state $S$, but now
the state evolves in time, resulting in our data
$
(S_1,Y_1),
(S_2,Y_2),
...,
(S_n,Y_n),
$

Thus the $Y_j$ are no longer iid.  Instead, the time pattern is assumed
to be \textit{Markovian}, or ``memoryless'':\footnote{Technically, the
Markovian nature of the $S_j$ does not imply the same for the $Y_j$.
For that, we need to assume, say, that conditionally on the $S_j$, the
random variables $Y_1, Y_2,...$ are independent.}

\begin{equation}
P(S_{k+1} = v_{k+1} ~|~ S_1 = v_1, S_2 = v_2, ...  S_k = v_k) =
P(S_{k+1} = v_{k+1} ~|~ S_k = v_k) 
\end{equation}

In English,

\begin{quote}
The probability of a future event, given the present and the past,
depends only on the present.
\end{quote}

The states $S_i$ are unobserved, i.e.\ ``hidden.''  Note that they 
may be real, as in our noisy network example, or just postulated, as in
the geyser example.  In our stock market example, for instance,
one might postulate ``bull'' and ``bear'' moods among the traders.

\subsection{The EM Algorithm}

The situation here is largely analogous to that of MMs.  For instance,
consider our battery example above:

\begin{itemize}

\item We again need a model for the distribution of $Y | S$.  In the
battery example, for instance, that could be exponential with parameter
$\lambda_S$.

\item The analog of the parameter $q$ is now the \textit{transition
matrix} of $S$, whose row $i$, column $j$ element is the $P(S_{m+1} = j
~|~ S_m = i)$.

\end{itemize} 

\subsection{The hmmr Package}

\begin{lstlisting}

> z <- hmmr::hmm(faithful$eruptions,2)
> summary(z)
Initial state probabilities model 
pr1 pr2 
  1   0 

Transition matrix 
        toS1  toS2
fromS1 0.479 0.521
fromS2 0.938 0.062

Response parameters 
Resp 1 : gaussian 
    Re1.(Intercept) Re1.sd
St1           4.289  0.413
St2           2.036  0.263

# z is an S4 class, one of whose components, posterior, is a data frame
> z@posterior$state
  [1] 1 2 1 2 1 2 1 1 2 1 2 1 1 2 1 2 2 1 2 1 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2
 [38] 1 2 1 1 2 1 2 1 1 1 2 1 2 1 1 2 1 2 1 1 2 1 1 2 1 2 1 2 1 1 1 2 1 1 2 1 1
 [75] 2 1 2 1 1 1 1 1 1 2 1 1 1 1 2 1 2 1 2 1 2 1 1 1 2 1 2 1 2 1 1 2 1 2 1 1 1
[112] 2 1 1 2 1 2 1 2 1 2 1 1 2 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 1 2 1 1 1 2 1 2
[149] 1 2 1 1 2 1 1 1 1 1 2 1 2 1 2 1 1 1 2 1 2 1 2 2 1 1 1 1 1 2 1 1 2 1 1 1 2
[186] 1 1 2 1 2 1 2 1 1 1 1 1 1 2 1 2 1 1 2 1 2 1 1 2 1 2 1 2 1 1 1 2 1 2 1 2 1
[223] 2 1 1 1 1 1 1 1 1 2 1 2 1 2 2 1 1 2 1 2 1 2 1 1 2 1 2 1 2 1 1 1 1 1 1 1 2
[260] 1 1 1 2 1 2 2 1 1 2 1 2 1

\end{lstlisting}

% Again, there may be a computability issue.  But see the examples below.

% \section{Example:  Cluster Analysis}
% 
% We will in Section \ref{mixclust} see that the notion of mixtures can
% also be applied to clustering.
% 
% \section{Markov Chain with Random $X_0$}
% 
% LOSE MARKOV PROPERTY
% 
% \section{Bayes Models, Including Empiricial Bayes}
% 
% JUST REFER TO THE BAYES SECTION, BUT NOTE IN THE LATTER THAT IT IS A
% MIXTURE

% \section{De Finetti's Theorem}
% 
% Consider again the trick coin example, Section \ref{oldtrick}.  As we
% have discussed, the tosses $B_i$ are not independent.  However, they are
% {\bf exchangeable}:
% 
% \begin{equation}
% P(B_
% \end{equation}


\end{document}

> x <- geyserSim(100000,0.9,0.3)
> z <- hmmr::hmm(x,2)
> summary(z)
Initial state probabilities model
pr1 pr2
  1   0

Transition matrix
        toS1  toS2
fromS1 0.681 0.319
fromS2 0.891 0.109

Response parameters
Resp 1 : gaussian
    Re1.(Intercept) Re1.sd
St1          -0.016  0.991
St2           1.959  1.018

geyserSim <- function(nTimePers,AtoB,BtoA)
{
   x <- vector(length=nTimePers)
   state <- 'A'
   for (i in 1:nTimePers) {  # could use rgeom() and while{} instead
      # check for state change
      if (state == 'A') {
         if (runif(1) < AtoB)  # now bad network
            state <- 'B'
      } else {  # state == 'B'
         if (runif(1) < BtoA)  # now bad network
            state <- 'A'
      }
      if (state == 'B') x[i] <- rnorm(1)
      else x[i] <- rnorm(1) + 2
   }
   x
}

netSim <- function(nBits,goodToBadProb,badToGoodProb,badProb0) 
{
   trueBits <- sample(0:1,nBits,replace=TRUE)
   rcvdBits <- trueBits  
   state <- 'good'
   for (i in 1:nBits) {  # could use rgeom() and while{} instead
      # check for state change 
      if (state == 'good') {
         if (runif(1) < goodToBadProb)  # now bad network
            state <- 'bad'
      } else {  # state == 'bad'
         if (runif(1) < badToGoodProb)  # now bad network
            state <- 'good'
      }
      if (state == 'bad' && runif(1) < badProb0)
         rcvdBits[i] <- 0
   }
   cbind(trueBits,rcvdBits)
}



